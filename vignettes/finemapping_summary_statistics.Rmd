---
title: "Fine-mapping with summary statistics"
author: "Yuxin Zou and Gao Wang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fine-mapping with summary statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,comment = "#",fig.width = 5,
                      fig.height = 3,fig.align = "center",
                      fig.cap = "&nbsp;",dpi = 120)
```

This vignette demonstrates how to use `susieR` with "summary statistics" in the context of genetic fine-mapping. 
We use the same simulated data as in [fine mapping vignette](finemapping.html). The simulated data is expression level of a gene ($y$) in $N \approx 600$ individuals. 
We want to identify with the genotype matrix $X_{N\times P}$ ($P=1001$) the genetic variables that causes changes in expression level. The data-set is shipped with `susieR`. It is simulated to have exactly 3 non-zero effects.

```{r}
library(susieR)
set.seed(1)
```

## The data-set

```{r}
data(N3finemapping)
attach(N3finemapping)
n = nrow(X)
```

Notice that we've simulated 2 sets of $Y$ as 2 simulation replicates. Here we'll focus on the first data-set.

```{r}
dim(Y)
```

Here are the 3 "true" signals in the first data-set:

```{r}
b <- true_coef[,1]
plot(b, pch=16, ylab='effect size')
```

```{r}
which(b != 0)
```

So the underlying causal variables are 403, 653 and 773.

## Summary statistics from simple regression

Summary statistics of genetic association studies typically contain effect size ($\hat{\beta}$ coefficient from regression), and p-value. 
These information can be used to perform fine-mapping with given an additional input of correlation matrix between variables.
The correlation matrix in genetics is typically referred to as LD matrix (LD for linkage disequilibrium). 
One may use external reference panels to estimate it when this matrix cannot be obtained from samples directly.
Caution that LD matrix here has to be correlation matrix $r$, not $r^2$ or $abs(r)$.

`univariate_regression` function can be used to compute 
summary statistics by fitting univariate simple regression variable by variable.
The results are $\hat{\beta}$ and $SE(\hat{\beta})$ from which z-scores can be derived. 
Alternatively you can obtain z-scores from $\hat{\beta}$ and p-values if you are provided with those information.
Again we focus only on results from the first data-set:

```{r}
sumstats <- univariate_regression(X, Y[,1])
z_scores <- sumstats$betahat / sumstats$sebetahat
susie_plot(z_scores, y = "z", b=b)
```

For this example the correlation matrix can be computed directly from data provide,

```{r}
R <- cor(X)
```

## Fine-mapping with `susieR` using summary statistics

We assume there are at most 10 causal variables, i.e. set `L = 10`, although SuSiE is generally robust to the choice of `L`.

Since the individual-level data is avialable for us, we have the in-sample LD matrix and the variance of $y$ is `r var(Y[,1])`. We fit SuSiE regression with summary statistics, $\hat{\beta}$, $SE(\hat{\beta})$, $R$, $n$, and var_y (they are sufficient statistics). We estimate the residual variance because we have the in-sample LD matrix.

```{r}
fitted_rss1 <- susie_rss(bhat = sumstats$betahat, shat = sumstats$sebetahat, n = n, R = R, var_y = var(Y[,1]), L = 10,
                         estimate_residual_variance = TRUE)
```

Using `summary` function, we can examine the posterior inclusion probability (PIP) for each variable, and the 95% credible sets. 

Here, we are the 95% credible sets.

```{r}
summary(fitted_rss1)$cs
```

The 3 causal signals have been captured by the 3 CS reported here. The
3rd CS contains many variables, including the true causal variable
403.

We can also plot the posterior inclusion probability (PIP),

```{r}
susie_plot(fitted_rss1, y="PIP", b=b)
```

The true causal variables are colored red. The 95% CS identified are circled in different colors.

The result is exactly same as using the individual level data with `susie`.

```{r}
fitted = susie(X, Y[,1], L = 10)
all.equal(fitted$pip, fitted_rss1$pip)
all.equal(coef(fitted)[-1], coef(fitted_rss1)[-1])
```

If the variance of y is unknown, we fit SuSiE regression with summary statistics, $\hat{\beta}$, $SE(\hat{\beta})$, $R$ and $n$ (or z-scores, $R$ and $n$). The output effect estimates are on the standardized $X$ and $y$ scale. Again, we estimate the residual variance because we have the in-sample LD matrix.

```{r}
fitted_rss2 = susie_rss(z = z_scores, R = R, n = n, L = 10, estimate_residual_variance = TRUE)
```

The result is same as using the individual level data with `susie`, but the output effect estimates are on different scale.
```{r}
all.equal(fitted$pip, fitted_rss2$pip)
plot(coef(fitted)[-1], coef(fitted_rss2)[-1], xlab = 'effects from SuSiE', ylab = 'effects from SuSiE-RSS', xlim=c(-1,1), ylim=c(-0.3,0.3))
```

The output effect estimates are same as applying SuSiE on standardized $X$ and $y, i.e. $y$ and the columns of $X$ are each divided by the sample standard deviation so that they each have the same standard deviation.

```{r}
fitted_standardize = susie(scale(X), scale(Y[,1]), L = 10)
all.equal(coef(fitted_standardize)[-1], coef(fitted_rss2)[-1])
```


## Fine-mapping with `susieR` using LD matrix from reference panel

When original genotype information is not available, one may use reference panel to estimate LD matrix.

We randomly generated 500 samples from $N(0,R)$ and treated them as reference panel genotype matrix `X_ref`.
```{r echo=F}
set.seed(1)
tmp = matrix(rnorm(500*1001), 500, 1001)
eigenR = eigen(R)
eigenR$values[eigenR$values < 1e-10] = 0
X_ref = tmp %*% (sqrt(eigenR$values) * t(eigenR$vectors))
R_ref = cor(X_ref)
```

We fit the SuSiE regression using out-of sample LD matrix. The residual variance is fixed as 1, because estimating residual variance sometimes produces very inaccurate estimates with out-of-sample LD matrix. The output effect estimates are on the standardized $X$ and $y$ scale.

```{r}
fitted_rss3 <- susie_rss(z_scores, R_ref, n=n, L = 10)
```

```{r}
susie_plot(fitted_rss3, y="PIP", b=b)
```
The model identifies the true signals, and the result is very similar to using the in-sample LD matrix, as the out-of-sample LD matrix is similar to the in-sample LD matrix in this example.

```{r, fig.width=6,fig.height=6}
plot(fitted_rss1$pip, fitted_rss3$pip, ylim=c(0,1), xlab='SuSiE PIP', ylab='SuSiE-RSS PIP')
```

In some rare cases, the sample size $n$ is unknown. We assume the sample size is infinity and all the effects are small, so the estimated PVE for each variant is close to zero. We fit the model using z-scores and out-of-sample LD matrix. The output effect estimates are on the noncentrality parameter scale.

```{r}
fitted_rss4 = susie_rss(z_scores, R_ref, L = 10)
```

```{r}
susie_plot(fitted_rss4, y="PIP", b=b)
```

## Session information

Here are some details about the computing environment, including the
versions of R, and the R packages, used to generate these results.

```{r}
sessionInfo()
```
